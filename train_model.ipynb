{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "+----+-------+--------------+-------------------+\n",
      "|year|country|article_amount|subject_area_abbrev|\n",
      "+----+-------+--------------+-------------------+\n",
      "|2023|  Spain|          68.0|               ECON|\n",
      "|2023|  Spain|          68.0|               ECON|\n",
      "|2023|  India|         190.0|               VETE|\n",
      "|2023|  India|         190.0|               VETE|\n",
      "|2023|  India|         190.0|               VETE|\n",
      "+----+-------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test\n",
      "+----+-------+--------------+-------------------+\n",
      "|year|country|article_amount|subject_area_abbrev|\n",
      "+----+-------+--------------+-------------------+\n",
      "|2024|   Iran|          50.0|               AGRI|\n",
      "|2024|   Iran|          50.0|               AGRI|\n",
      "|2024|   Iran|          50.0|               AGRI|\n",
      "|2024|   Iran|          50.0|               AGRI|\n",
      "|2024|   Iran|          50.0|               AGRI|\n",
      "+----+-------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize SparkSession\n",
    "# spark.stop()    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Step 2: Read CSV File\n",
    "file_path_train = \"./csv_file/combined_data_2018_2023.csv\"  # Replace with your file path\n",
    "file_path_test = \"./csv_file/final_data_prep_2024.csv\"  # Replace with your file path\n",
    "df_train = spark.read.csv(file_path_train, header=True, inferSchema=True)\n",
    "df_test = spark.read.csv(file_path_test, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Train\")\n",
    "df_train.show(5)\n",
    "\n",
    "print(\"Test\")\n",
    "df_test.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-2023 : \n",
      "Unique values in column 'year': 6\n",
      "Unique values in column 'country': 175\n",
      "Unique values in column 'article_amount': 467\n",
      "Unique values in column 'subject_area_abbrev': 27\n"
     ]
    }
   ],
   "source": [
    "print(\"2018-2023 : \")\n",
    "for column in df_train.columns:\n",
    "    unique_count = df_train.select(column).distinct().count()\n",
    "    print(f\"Unique values in column '{column}': {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 :\n",
      "Unique values in column 'year': 1\n",
      "Unique values in column 'country': 28\n",
      "Unique values in column 'article_amount': 109\n",
      "Unique values in column 'subject_area_abbrev': 27\n"
     ]
    }
   ],
   "source": [
    "print(\"2024 :\")\n",
    "for column in df_test.columns:\n",
    "    unique_count = df_test.select(column).distinct().count()\n",
    "    print(f\"Unique values in column '{column}': {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Schema:\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- article_amount: double (nullable = true)\n",
      " |-- subject_area_abbrev: string (nullable = true)\n",
      "\n",
      "Testing Data Schema:\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- article_amount: double (nullable = true)\n",
      " |-- subject_area_abbrev: string (nullable = true)\n",
      "\n",
      "Predictions have been exported to predictions_output.csv.\n",
      "Metrics have been exported to metrics_output.csv.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataSciencePipeline\").getOrCreate()\n",
    "\n",
    "# # File paths for training and testing datasets\n",
    "# file_path_train = \"path/to/train_file.csv\"  # Replace with your actual train file path\n",
    "# file_path_test = \"path/to/test_file.csv\"    # Replace with your actual test file path\n",
    "\n",
    "try:\n",
    "    # Read training and testing datasets without dropping duplicates\n",
    "    df_train = spark.read.csv(file_path_train, header=True, inferSchema=True)\n",
    "    df_test = spark.read.csv(file_path_test, header=True, inferSchema=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    spark.stop()\n",
    "    raise\n",
    "\n",
    "# Print schemas to confirm structure\n",
    "print(\"Training Data Schema:\")\n",
    "df_train.printSchema()\n",
    "print(\"Testing Data Schema:\")\n",
    "df_test.printSchema()\n",
    "\n",
    "# Ensure critical columns are present\n",
    "required_columns = [\"year\", \"article_amount\", \"subject_area_abbrev\", \"country\"]\n",
    "missing_columns_train = [col for col in required_columns if col not in df_train.columns]\n",
    "missing_columns_test = [col for col in required_columns if col not in df_test.columns]\n",
    "\n",
    "if missing_columns_train or missing_columns_test:\n",
    "    raise ValueError(f\"Missing required columns. Train: {missing_columns_train}, Test: {missing_columns_test}\")\n",
    "\n",
    "# Handle missing values\n",
    "df_train = df_train.dropna(subset=required_columns)\n",
    "df_test = df_test.dropna(subset=required_columns)\n",
    "\n",
    "# Normalize country column to avoid mismatches\n",
    "df_train = df_train.withColumn(\"country\", F.trim(F.lower(F.col(\"country\"))))\n",
    "df_test = df_test.withColumn(\"country\", F.trim(F.lower(F.col(\"country\"))))\n",
    "\n",
    "# Filter datasets to have only common countries between train and test\n",
    "common_countries = df_train.select(\"country\").distinct().intersect(df_test.select(\"country\").distinct())\n",
    "df_train = df_train.join(common_countries, on=\"country\", how=\"inner\")\n",
    "df_test = df_test.join(common_countries, on=\"country\", how=\"inner\")\n",
    "\n",
    "# Handle categorical features\n",
    "subject_area_indexer = StringIndexer(inputCol=\"subject_area_abbrev\", outputCol=\"subject_area_indexed\", handleInvalid=\"skip\")\n",
    "country_indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_indexed\", handleInvalid=\"skip\")\n",
    "\n",
    "# Assemble feature columns\n",
    "feature_columns = [\"year\", \"article_amount\", \"subject_area_indexed\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# RandomForestClassifier setup\n",
    "rf = RandomForestClassifier(labelCol=\"country_indexed\", featuresCol=\"features\", probabilityCol=\"probability\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[subject_area_indexer, country_indexer, assembler, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(df_train)\n",
    "\n",
    "# Make predictions\n",
    "df_test_predictions = model.transform(df_test)\n",
    "\n",
    "# Display predictions (for export purposes)\n",
    "df_test_predictions = df_test_predictions.select(\n",
    "    \"country\", \"article_amount\", \"subject_area_abbrev\", \"features\",\n",
    "    \"country_indexed\", \"prediction\", \"probability\"\n",
    ")\n",
    "\n",
    "# Convert the predictions dataframe to Pandas for export\n",
    "df_test_predictions_pd = df_test_predictions.toPandas()\n",
    "\n",
    "# Export predictions to CSV using Pandas\n",
    "predictions_output_path = \"predictions_output.csv\"  # Specify the path where you want to save the CSV file\n",
    "df_test_predictions_pd.to_csv(predictions_output_path, index=False)\n",
    "\n",
    "print(f\"Predictions have been exported to {predictions_output_path}.\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"country_indexed\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = evaluator.evaluate(df_test_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1_score = evaluator.evaluate(df_test_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Store metrics in a list for export\n",
    "metrics = [\n",
    "    {\"metric\": \"accuracy\", \"value\": accuracy},\n",
    "    {\"metric\": \"f1_score\", \"value\": f1_score},\n",
    "]\n",
    "\n",
    "# Evaluate precision and recall\n",
    "for metric in [\"weightedPrecision\", \"weightedRecall\"]:\n",
    "    score = evaluator.evaluate(df_test_predictions, {evaluator.metricName: metric})\n",
    "    metrics.append({\"metric\": metric, \"value\": score})\n",
    "\n",
    "# Convert metrics to a Pandas DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Export metrics to CSV using Pandas\n",
    "metrics_output_path = \"metrics_output.csv\"  # Specify the path where you want to save the CSV file\n",
    "metrics_df.to_csv(metrics_output_path, index=False)\n",
    "\n",
    "print(f\"Metrics have been exported to {metrics_output_path}.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
