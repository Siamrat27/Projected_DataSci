{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+----+--------+--------------+-----------+\n",
      "|        Source Title|                 DOI|               EID|Year|Abstract|Subjected Area|    Country|\n",
      "+--------------------+--------------------+------------------+----+--------+--------------+-----------+\n",
      "|Recoletos Multidi...|10.32871/rmrj1806...|2-s2.0-85166929946|2018|    NULL|          MULT|Philippines|\n",
      "|Recoletos Multidi...|10.32871/rmrj1806...|2-s2.0-85160651147|2018|    NULL|          MULT|Philippines|\n",
      "|Recoletos Multidi...|10.32871/rmrj1806...|2-s2.0-85160637511|2018|    NULL|          MULT|Philippines|\n",
      "|Recoletos Multidi...|10.32871/rmrj1806...|2-s2.0-85160636983|2018|    NULL|          MULT|Philippines|\n",
      "|Recoletos Multidi...|10.32871/rmrj1806...|2-s2.0-85153625255|2018|    NULL|          MULT|Philippines|\n",
      "+--------------------+--------------------+------------------+----+--------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+------------------+----+--------+--------------+--------------+\n",
      "|                 DOI|               EID|Year|Abstract|Subjected Area|       Country|\n",
      "+--------------------+------------------+----+--------+--------------+--------------+\n",
      "|10.1016/j.isci.20...|2-s2.0-85209659281|2024|    NULL|          MULT|        Canada|\n",
      "|10.1016/j.isci.20...|2-s2.0-85209657200|2024|    NULL|          MULT|         China|\n",
      "|10.1016/j.isci.20...|2-s2.0-85209657050|2024|    NULL|          MULT|         India|\n",
      "|10.1016/j.isci.20...|2-s2.0-85209655100|2024|    NULL|          MULT|United Kingdom|\n",
      "|10.1016/j.isci.20...|2-s2.0-85209649906|2024|    NULL|          MULT|   Netherlands|\n",
      "+--------------------+------------------+----+--------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize SparkSession\n",
    "# spark.stop()    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Step 2: Read CSV File\n",
    "file_path = \"./csv_file/scopus_data_2018_2023.csv\"  # Replace with your file path\n",
    "df1 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "file_path = \"./csv_file/scopus_2024_1000_all_subjects_cleaned.csv\"  # Replace with your file path\n",
    "df2 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Show the DataFrame\n",
    "df1.show(5) \n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Unneccesary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 356451 7\n",
      "after 356451 4\n",
      "+--------------------+----+--------------+-----------+\n",
      "|                 DOI|Year|Subjected Area|    Country|\n",
      "+--------------------+----+--------------+-----------+\n",
      "|10.32871/rmrj1806...|2018|          MULT|Philippines|\n",
      "|10.32871/rmrj1806...|2018|          MULT|Philippines|\n",
      "|10.32871/rmrj1806...|2018|          MULT|Philippines|\n",
      "|10.32871/rmrj1806...|2018|          MULT|Philippines|\n",
      "|10.32871/rmrj1806...|2018|          MULT|Philippines|\n",
      "+--------------------+----+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "before 41695 6\n",
      "after 41695 4\n",
      "+--------------------+----+--------------+--------------+\n",
      "|                 DOI|Year|Subjected Area|       Country|\n",
      "+--------------------+----+--------------+--------------+\n",
      "|10.1016/j.isci.20...|2024|          MULT|        Canada|\n",
      "|10.1016/j.isci.20...|2024|          MULT|         China|\n",
      "|10.1016/j.isci.20...|2024|          MULT|         India|\n",
      "|10.1016/j.isci.20...|2024|          MULT|United Kingdom|\n",
      "|10.1016/j.isci.20...|2024|          MULT|   Netherlands|\n",
      "+--------------------+----+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"before \" + str(df1.count()) + \" \" + str(len(df1.columns)))\n",
    "df1 = df1.drop('Title', 'EID','Source Title','Abstract')\n",
    "\n",
    "# Drop rows with non-null values below 50% of the total columns\n",
    "thresh = int(0.5 * len(df1.columns))\n",
    "df1 = df1.dropna(thresh=thresh)\n",
    "df_clean = df1.dropna()\n",
    "\n",
    "print(\"after \" +str(df1.count()) + \" \" + str(len(df1.columns)))\n",
    "df1.show(5)\n",
    "\n",
    "print(\"before \" + str(df2.count()) + \" \" + str(len(df2.columns)))\n",
    "df2 = df2.drop('Title', 'EID','Source Title','Abstract')\n",
    "\n",
    "# Drop rows with non-null values below 50% of the total columns\n",
    "thresh = int(0.5 * len(df2.columns))\n",
    "df2 = df2.dropna(thresh=thresh)\n",
    "df_clean = df2.dropna()\n",
    "\n",
    "print(\"after \" +str(df2.count()) + \" \" + str(len(df2.columns)))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Country,Subjected Area,Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----+-------------+\n",
      "|Country|Subjected Area|Year|Article Count|\n",
      "+-------+--------------+----+-------------+\n",
      "|  China|          ENER|2021|         1374|\n",
      "|  China|          MATE|2021|         1331|\n",
      "|  China|          MATE|2022|         1275|\n",
      "|  China|          ENGI|2021|         1268|\n",
      "|  China|          ENER|2022|         1238|\n",
      "+-------+--------------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------+----+-----------------+-------------+\n",
      "|Country|Subjected Area|Year|              DOI|Article Count|\n",
      "+-------+--------------+----+-----------------+-------------+\n",
      "| Poland|          SOCI|2018|10.3390/w11010049|           21|\n",
      "| Poland|          SOCI|2018|10.3390/w11010056|           21|\n",
      "| Taiwan|          COMP|2019|             NULL|           20|\n",
      "|  Spain|          ECON|2023|             NULL|           68|\n",
      "|  Spain|          ECON|2023|             NULL|           68|\n",
      "+-------+--------------+----+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(df.select(df['EID']).distinct().count())\n",
    "\n",
    "df_article = df1.groupBy('Country', 'Subjected Area', 'Year') \\\n",
    "    .agg(F.count('DOI').alias('Article Count'))\n",
    "\n",
    "df_article_sorted = df_article.orderBy(F.desc('Article Count'))\n",
    "\n",
    "# Show the aggregated data\n",
    "df_article_sorted.show(5)\n",
    "\n",
    "# Perform the join operation between df_article and df_clean\n",
    "df1 = df1.join(df_article, \n",
    "                    on=['Country', 'Subjected Area', 'Year'], \n",
    "                    how='inner')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "# df_joined.show()\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----+-------------+\n",
      "|Country|Subjected Area|Year|Article Count|\n",
      "+-------+--------------+----+-------------+\n",
      "|  China|          CHEM|2024|         1042|\n",
      "|  China|          CENG|2024|         1038|\n",
      "|  China|          ENGI|2024|          987|\n",
      "|  China|          MATE|2024|          921|\n",
      "|  China|          ENER|2024|          906|\n",
      "+-------+--------------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------+----+--------------------+-------------+\n",
      "|       Country|Subjected Area|Year|                 DOI|Article Count|\n",
      "+--------------+--------------+----+--------------------+-------------+\n",
      "|        Canada|          MULT|2024|10.1016/j.isci.20...|           21|\n",
      "|         China|          MULT|2024|10.1016/j.isci.20...|          493|\n",
      "|         India|          MULT|2024|10.1016/j.isci.20...|           91|\n",
      "|United Kingdom|          MULT|2024|10.1016/j.isci.20...|           29|\n",
      "|   Netherlands|          MULT|2024|10.1016/j.isci.20...|           17|\n",
      "+--------------+--------------+----+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_article = df2.groupBy('Country', 'Subjected Area', 'Year') \\\n",
    "    .agg(F.count('DOI').alias('Article Count'))\n",
    "\n",
    "df_article_sorted = df_article.orderBy(F.desc('Article Count'))\n",
    "\n",
    "# Show the aggregated data\n",
    "df_article_sorted.show(5)\n",
    "# Perform the join operation between df_article and df_clean\n",
    "df2 = df2.join(df_article, \n",
    "                    on=['Country', 'Subjected Area', 'Year'], \n",
    "                    how='inner')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "# df_joined.show()\n",
    "df2.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+-------------------+-------+--------------+\n",
      "|              doi|year|subject_area_abbrev|country|article_amount|\n",
      "+-----------------+----+-------------------+-------+--------------+\n",
      "|10.3390/w11010049|2018|               SOCI| Poland|            21|\n",
      "|10.3390/w11010056|2018|               SOCI| Poland|            21|\n",
      "|             NULL|2019|               COMP| Taiwan|            20|\n",
      "|             NULL|2023|               ECON|  Spain|            68|\n",
      "|             NULL|2023|               ECON|  Spain|            68|\n",
      "+-----------------+----+-------------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+----+-------------------+--------------+--------------+\n",
      "|                 doi|year|subject_area_abbrev|       country|article_amount|\n",
      "+--------------------+----+-------------------+--------------+--------------+\n",
      "|10.1016/j.isci.20...|2024|               MULT|        Canada|            21|\n",
      "|10.1016/j.isci.20...|2024|               MULT|         China|           493|\n",
      "|10.1016/j.isci.20...|2024|               MULT|         India|            91|\n",
      "|10.1016/j.isci.20...|2024|               MULT|United Kingdom|            29|\n",
      "|10.1016/j.isci.20...|2024|               MULT|   Netherlands|            17|\n",
      "+--------------------+----+-------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.select(\n",
    "    df1['DOI'].alias('doi'),\n",
    "    df1['Year'].alias('year'),\n",
    "    df1['Subjected Area'].alias('subject_area_abbrev'),\n",
    "    df1['Country'].alias('country'),\n",
    "    df1['Article Count'].alias('article_amount')\n",
    ")\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df1.show(5)\n",
    "\n",
    "df2 = df2.select(\n",
    "    df2['DOI'].alias('doi'),\n",
    "    df2['Year'].alias('year'),\n",
    "    df2['Subjected Area'].alias('subject_area_abbrev'),\n",
    "    df2['Country'].alias('country'),\n",
    "    df2['Article Count'].alias('article_amount')\n",
    ")\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+-------------------+\n",
      "|year|country|article_amount|subject_area_abbrev|\n",
      "+----+-------+--------------+-------------------+\n",
      "|2018| Poland|            21|               SOCI|\n",
      "|2018| Poland|            21|               SOCI|\n",
      "|2019| Taiwan|            20|               COMP|\n",
      "|2023|  Spain|            68|               ECON|\n",
      "|2023|  Spain|            68|               ECON|\n",
      "+----+-------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-------+--------------+-------------------+\n",
      "|year|country|article_amount|subject_area_abbrev|\n",
      "+----+-------+--------------+-------------------+\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "+----+-------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_column_order = ['year', 'country','article_amount', 'subject_area_abbrev']\n",
    "\n",
    "# Reorder the DataFrame\n",
    "df1 = df1.select(*new_column_order)\n",
    "df1.show(5)\n",
    "\n",
    "df2 = df2.select(*new_column_order)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "NEUR\n",
      "297.8949403242188\n",
      "China\n",
      "NEUR\n",
      "333.16440820242235\n"
     ]
    }
   ],
   "source": [
    "country_mode_value = df1.groupBy('country').count().orderBy(F.desc('count')).first()[0]\n",
    "print(country_mode_value)\n",
    "\n",
    "subject_mode_value = df1.groupBy('subject_area_abbrev').count().orderBy(F.desc('count')).first()[0]\n",
    "print(subject_mode_value)\n",
    "\n",
    "article_mean_value = df1.agg(F.avg('article_amount').alias('mean_article_amount')).collect()[0]['mean_article_amount']\n",
    "print(article_mean_value)\n",
    "\n",
    "df = df1.na.fill(\n",
    "    {\n",
    "        'country' : country_mode_value, \n",
    "        'subject_area_abbrev': subject_mode_value,\n",
    "        'article_amount': article_mean_value,\n",
    "    \n",
    "    })\n",
    "\n",
    "country_mode_value = df2.groupBy('country').count().orderBy(F.desc('count')).first()[0]\n",
    "print(country_mode_value)\n",
    "\n",
    "subject_mode_value = df2.groupBy('subject_area_abbrev').count().orderBy(F.desc('count')).first()[0]\n",
    "print(subject_mode_value)\n",
    "\n",
    "article_mean_value = df2.agg(F.avg('article_amount').alias('mean_article_amount')).collect()[0]['mean_article_amount']\n",
    "print(article_mean_value)\n",
    "\n",
    "df = df2.na.fill(\n",
    "    {\n",
    "        'country' : country_mode_value, \n",
    "        'subject_area_abbrev': subject_mode_value,\n",
    "        'article_amount': article_mean_value,\n",
    "    \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-2023 : \n",
      "Unique values in column 'year': 6\n",
      "Unique values in column 'country': 200\n",
      "Unique values in column 'article_amount': 419\n",
      "Unique values in column 'subject_area_abbrev': 27\n",
      "2024 : \n",
      "Unique values in column 'year': 1\n",
      "Unique values in column 'country': 155\n",
      "Unique values in column 'article_amount': 133\n",
      "Unique values in column 'subject_area_abbrev': 27\n"
     ]
    }
   ],
   "source": [
    "# Count unique values for each column\n",
    "print(\"2018-2023 : \")\n",
    "for column in df1.columns:\n",
    "    unique_count = df1.select(column).distinct().count()\n",
    "    print(f\"Unique values in column '{column}': {unique_count}\")\n",
    "\n",
    "print(\"2024 : \")\n",
    "for column in df2.columns:\n",
    "    unique_count = df2.select(column).distinct().count()\n",
    "    print(f\"Unique values in column '{column}': {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+-------------------+\n",
      "|year|country|article_amount|subject_area_abbrev|\n",
      "+----+-------+--------------+-------------------+\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "|2024|   Iran|            50|               AGRI|\n",
      "+----+-------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "file_path_new = r\"C:\\Users\\admin\\Downloads\\Data 2018-2023\\Project\\csv_file\\2018_2023_data_prep_starter.csv\"\n",
    "\n",
    "df_new = spark.read.csv(file_path_new, header=True, inferSchema=True)\n",
    "\n",
    "df2 = df2.union(df_new)\n",
    "\n",
    "df2.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove remove outlier and Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Percentile for article_amount: 25.0\n",
      "1st Percentile for article_amount: 18.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def remove_outliers_left(df, col):\n",
    "    # Convert the column to DoubleType\n",
    "    df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
    "    \n",
    "    # Calculate the 1st percentile (1%) of the specified column\n",
    "    percentile_1 = df.approxQuantile(col, [0.20], 0.0)[0]\n",
    "    print(f\"1st Percentile for {col}: {percentile_1}\")\n",
    "    \n",
    "    # Filter out values less than the 1st percentile\n",
    "    # return df.filter(F.col(col) >= 50)\n",
    "    return df.filter(F.col(col) >= percentile_1)\n",
    "\n",
    "# Apply the function to remove left-side outliers (below 1st percentile)\n",
    "df1 = remove_outliers_left(df1, \"article_amount\")\n",
    "df2 = remove_outliers_left(df2, \"article_amount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been exported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Folder where the CSV files will be saved\n",
    "folder_path = '/csv_file/'  # Specify the folder path where you want to save the CSV files\n",
    "\n",
    "# Ensure the folder exists (create if it doesn't)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Convert the DataFrames to Pandas\n",
    "pandas_df1 = df1.toPandas()\n",
    "pandas_df2 = df2.toPandas()\n",
    "\n",
    "# Export to CSV files in the specified folder\n",
    "pandas_df1.to_csv(os.path.join(folder_path, 'final_data_prep_2018-2023.csv'), index=False)\n",
    "pandas_df2.to_csv(os.path.join(folder_path, 'final_data_prep_2024.csv'), index=False)\n",
    "\n",
    "print(\"CSV files have been exported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to /csv_file/combined_data_2018_2023.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "file1 = \"./csv_file/final_data_prep_2018-2023.csv\"\n",
    "file2 = \"./csv_file/2018_2023_data_prep_starter.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Define the output folder and file path\n",
    "folder_path = '/csv_file/'  # Specify the folder path where the combined file will be saved\n",
    "output_file = os.path.join(folder_path, 'combined_data_2018_2023.csv')\n",
    "\n",
    "# Ensure the folder exists (create if it doesn't)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Export the combined data to CSV\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
